Traceback (most recent call last):
  File "/home/aiotlab/projects/cuongdm/MEC1/mec.py", line 105, in <module>
    value_loss.append(MSELoss()(model.v(s_vec.to(device)).reshape(-1), td_target_vec))
  File "/home/aiotlab/projects/cuongdm/MEC1/utils/A2C.py", line 25, in v
    v = self.model.v(x)
  File "/home/aiotlab/projects/cuongdm/MEC1/models/model.py", line 79, in v
    out_shared_layers = self.shared_layers(x)
  File "/home/aiotlab/projects/cuongdm/MEC1/models/model.py", line 53, in shared_layers
    out_conv1 = self.pool(self.relu(self.batchnorm1(self.conv1(xi))))
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 98, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/aiotlab/anaconda3/envs/thanhnt_vaipe/lib/python3.9/site-packages/torch/nn/functional.py", line 1299, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 9.78 GiB total capacity; 1.56 GiB already allocated; 14.88 MiB free; 1.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF